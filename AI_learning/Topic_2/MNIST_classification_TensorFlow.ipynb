{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Byl2KCgMNl"
      },
      "source": [
        "# Handwritten digits classification using TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCKj0Bn4gMNt"
      },
      "source": [
        "Putting all the concepts we have learned so far, we will see how can use tensorflow to\n",
        "build a neural network to recognize handwritten digits. If you are playing around deep\n",
        "learning off late then you must have come across MNIST dataset. It is being called the hello\n",
        "world of deep learning.\n",
        "\n",
        "It consists of 55,000 data points of handwritten digits (0 to 9).\n",
        "In this section, we will see how can we use our neural network to recognize the\n",
        "handwritten digits and also we will get hang of tensorflow and tensorboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIjW-6rRgMNu"
      },
      "source": [
        "## Import required libraries\n",
        "\n",
        "As a first step, let us import all the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "upbsikQ1gMNw"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Display the first image in the training data\n",
        "plt.imshow(x_train[7], cmap='gray')\n",
        "plt.title(f\"Label: {y_train[0]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TZ7CXfqXgMNy",
        "outputId": "322d4591-f682-457e-8c41-0cdac4cfb2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJW-Y-4gMNz"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "In the below code, \"data/mnist\" implies the location where we store the MNIST dataset.\n",
        "one_hot=True implies we are one-hot encoding the labels (0 to 9):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1vr8pQDqgMN0",
        "outputId": "bb6f4df1-4583-49db-a966-61ed399ae249"
      },
      "outputs": [],
      "source": [
        "\n",
        "mnist = tf.keras.datasets.mnist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyTPNhNugMN1"
      },
      "source": [
        "Let's check what we got in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "oQ7d0fDVgMN2",
        "outputId": "403e02b0-1b4c-4993-d8ce-ba5f39b99795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in training set: 60000\n",
            "Number of labels in training set: 60000\n",
            "Number of images in test set: 10000\n",
            "Number of labels in test set: 10000\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding if needed\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "# Print the number of images and labels in the training and test sets\n",
        "print(\"Number of images in training set: {}\".format(x_train.shape[0]))\n",
        "print(\"Number of labels in training set: {}\".format(y_train.shape[0]))\n",
        "\n",
        "print(\"Number of images in test set: {}\".format(x_test.shape[0]))\n",
        "print(\"Number of labels in test set: {}\".format(y_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzYV0C64gMN2"
      },
      "source": [
        "We have 55,000 images in the training set and each image is of size 784 and we have 10 labels which are actually 0 to 9. Similarly, we have 10000 images in the test set.\n",
        "\n",
        "Now we plot one image to see how it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "N8V87y2zgMN3",
        "outputId": "89794a16-6f2c-4a14-c9e2-aa221e4dfe7a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img1 = x_train[123].reshape(28, 28)\n",
        "#reshape(28, 28) reshapes the 1-dimensional array representing the image into a 2-dimensional (28x28) array. \n",
        "# This is necessary because the images in MNIST dataset are flattened into 1D arrays (784 pixels).\n",
        "plt.imshow(img1, cmap='Greys')\n",
        "#cmap='Greys' specifies the colormap to use for displaying the image. 'Greys' colormap displays the image in grayscale.\n",
        "plt.title(f\"Label: {y_train[123]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprUThESgMN3"
      },
      "source": [
        "## Define the number of neurons in each layer\n",
        "\n",
        "We build a 4 layer neural network with 3 hidden layers and 1 output layer. As the size of\n",
        "the input image is 784. We set the num_input to 784 and since we have 10 handwritten\n",
        "digits (0 to 9), We set 10 neurons in the output layer. We define the number of neurons in\n",
        "each layer as follows,\n",
        "\n",
        "Hidden Layers: Perform computations to transform the input data into a format that makes it easier for the network to learn patterns and relationships in the data.\n",
        "\n",
        "Output Layer: Produces the final predictions or classifications for the input data.\n",
        "## Training Context:\n",
        "During training, the neural network adjusts the weights and biases of each neuron through an optimization process (e.g., using gradient descent and backpropagation). The goal is to minimize a defined loss function (like cross-entropy in classification tasks) and improve the accuracy of predictions on the training data.\n",
        "The specified architecture (number of layers and neurons) influences the capacity of the model to learn complex patterns in the data. Deeper networks with more neurons can potentially capture more intricate relationships but require more computational resources and may be prone to overfitting if not properly regularized.\n",
        "## Summary:\n",
        "In summary, specifying the number of neurons in each layer defines the architecture of a neural network model. Each layer's configuration influences how the model processes and learns from the input data, leading to the model's ability to generalize and make accurate predictions on unseen data during training and evaluation phases. Adjusting these parameters involves balancing model complexity, computational efficiency, and performance metrics such as accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "yOVXRgD1gMN4"
      },
      "outputs": [],
      "source": [
        "#number of neurons in input layer\n",
        "num_input = 784\n",
        "\n",
        "#number of neurons in hidden layer 1\n",
        "num_hidden1 = 256\n",
        "\n",
        "#number of neurons in hidden layer 2\n",
        "num_hidden2 = 128\n",
        "\n",
        "#number of neurons in hidden layer 3\n",
        "num_hidden3 = 64\n",
        "\n",
        "#number of neurons in output layer\n",
        "num_output = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfPvk5WIgMN5"
      },
      "source": [
        "## Defining placeholders\n",
        "\n",
        "As we learned, we first need to define the placeholders for input and output. Values for\n",
        "the placeholders will be feed at the run time through feed_dict:\n",
        "\n",
        "\n",
        "In summary, the provided code sets up input and output placeholders (X and Y) using TensorFlow's Keras API. These placeholders define the shape and structure of the input and output data expected by the neural network model. Organizing these placeholders within tf.name_scope helps maintain clarity and organization within the computational graph, which is crucial for managing larger and more complex models.\n",
        "\n",
        "Input : data with array has 784\n",
        "Output : data with array has 10 , from 0 to 9 to detect the number in the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "MLB87He1gMN6"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('input'):\n",
        "    X = tf.keras.Input(shape=(num_input,), name='X')\n",
        "\n",
        "with tf.name_scope('output'):\n",
        "    Y = tf.keras.Input(shape=(num_output,), name='Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvSUyYFugMN6"
      },
      "source": [
        "Since we have a 4 layer network, we have 4 weights and 4 baises. We initialize our weights\n",
        "by drawing values from the truncated normal distribution with a standard deviation of\n",
        "0.1.\n",
        "\n",
        "Remember, the dimensions of the weights matrix should be a number of neurons in the\n",
        "previous layer x number of neurons in the current layer. For instance, the dimension of\n",
        "weight matrix w3 should be the number of neurons in the hidden layer 2 x number of\n",
        "neurons in hidden layer 3.\n",
        "\n",
        "We often define all the weights in a dictionary as given below:\n",
        "\n",
        "## Step 1 \n",
        "with tf.name_scope('weights')::\n",
        "\n",
        "tf.name_scope is used to group operations and variables within TensorFlow, providing better visualization and organization in the computational graph.\n",
        "Here, it encapsulates the definition of weights (weights['w1'], weights['w2'], weights['w3'], and weights['out']) for the neural network.\n",
        "\n",
        "## Step 2 \n",
        "Weights Initialization:\n",
        "\n",
        "'w1': Represents the weights connecting the input layer to the first hidden layer.\n",
        "\n",
        "Shape: [num_input, num_hidden1]\n",
        "tf.random.truncated_normal: Initializes the weights with values drawn from a truncated normal distribution.\n",
        "stddev=0.1: Specifies the standard deviation of the distribution.\n",
        "name='weight_1': Assigns a name to this variable in TensorFlow.\n",
        "\n",
        "\n",
        "'w2': Represents the weights connecting the first hidden layer to the second hidden layer.\n",
        "\n",
        "Shape: [num_hidden1, num_hidden2]\n",
        "Similar initialization and naming conventions as 'w1'.\n",
        "\n",
        "'w3': Represents the weights connecting the second hidden layer to the third hidden layer.\n",
        "\n",
        "Shape: [num_hidden2, num_hidden3]\n",
        "Similar initialization and naming conventions.\n",
        "'out': Represents the weights connecting the third hidden layer to the output layer.\n",
        "\n",
        "Shape: [num_hidden3, num_output]\n",
        "Similar initialization and naming conventions.\n",
        "\n",
        "## Purpose \n",
        "\n",
        "Weights (weights):\n",
        "\n",
        "These variables ('w1', 'w2', 'w3', 'out') store the connection weights between layers in a neural network.\n",
        "They are critical parameters that the network learns during training to optimize performance on a given task (e.g., classification).\n",
        "Initialized using tf.random.truncated_normal, ensuring that weights start with small random values, which helps prevent the network from getting stuck during training.\n",
        "TensorFlow Variable:\n",
        "\n",
        "Each weight is defined as a TensorFlow Variable, which means its value can be modified during training to minimize the loss function (i.e., the difference between predicted and actual outputs).\n",
        "\n",
        "## Summary \n",
        "The provided code initializes and organizes weights for a neural network using TensorFlow. Each weight ('w1', 'w2', 'w3', 'out') connects specific layers of the network and is essential for learning and making predictions. Organizing these weights within tf.name_scope helps maintain clarity and structure in the computational graph, facilitating easier management and debugging of complex models.\n",
        "\n",
        "## Output meaning \n",
        "\n",
        "- Each element in this matrix (array) represents the strength of the connection (weight) between a specific input neuron and a specific neuron in the first hidden layer.\n",
        "- The values are initialized using a truncated normal distribution with a standard deviation of 0.1, meaning they are small random numbers centered around zero.\n",
        "- During training, these weights will be adjusted through backpropagation to minimize the loss function, helping the neural network learn patterns in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "HBLwqaL_gMN6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'w1': <tf.Variable 'weights/weight_1:0' shape=(784, 256) dtype=float32, numpy=\n",
            "array([[-0.10601294,  0.15786554,  0.13444538, ...,  0.19747466,\n",
            "         0.1251301 ,  0.07689772],\n",
            "       [-0.15778245, -0.06543959,  0.09584814, ...,  0.02265877,\n",
            "         0.00513231,  0.04495052],\n",
            "       [ 0.14997043,  0.080705  , -0.16437049, ...,  0.01410505,\n",
            "         0.09432485,  0.06860576],\n",
            "       ...,\n",
            "       [ 0.09020429, -0.01976741,  0.05815829, ..., -0.10513723,\n",
            "         0.06359536, -0.05843518],\n",
            "       [-0.15192904,  0.05339912,  0.0148823 , ..., -0.02586138,\n",
            "         0.07445789,  0.05971372],\n",
            "       [-0.14789155, -0.18922776,  0.00361359, ...,  0.00646393,\n",
            "         0.08639098, -0.00982498]], dtype=float32)>, 'w2': <tf.Variable 'weights/weight_2:0' shape=(256, 128) dtype=float32, numpy=\n",
            "array([[-0.03035381, -0.14842565, -0.05262039, ...,  0.0389008 ,\n",
            "        -0.04062995,  0.08781775],\n",
            "       [ 0.04659995,  0.03598044,  0.04133796, ...,  0.0140517 ,\n",
            "         0.00040473, -0.0102139 ],\n",
            "       [-0.04466478,  0.182686  , -0.14417242, ...,  0.007802  ,\n",
            "        -0.12864667,  0.04429647],\n",
            "       ...,\n",
            "       [-0.13055687,  0.01822247, -0.04937786, ..., -0.13795076,\n",
            "         0.11500335,  0.0281004 ],\n",
            "       [-0.00950728,  0.05730844,  0.06394019, ...,  0.04136552,\n",
            "        -0.01426601,  0.00431414],\n",
            "       [-0.02389732, -0.07119716,  0.03372483, ...,  0.07290228,\n",
            "        -0.08285945, -0.04391576]], dtype=float32)>, 'w3': <tf.Variable 'weights/weight_3:0' shape=(128, 64) dtype=float32, numpy=\n",
            "array([[ 0.01454453, -0.01682318,  0.16784102, ..., -0.12646595,\n",
            "         0.03973908, -0.07044211],\n",
            "       [ 0.07951719,  0.02224736,  0.08963562, ..., -0.06880533,\n",
            "        -0.0964915 , -0.10486936],\n",
            "       [ 0.13707267,  0.05504766,  0.12452247, ..., -0.01626661,\n",
            "         0.05697423,  0.04732406],\n",
            "       ...,\n",
            "       [ 0.00443191, -0.00708373, -0.1504731 , ..., -0.07848068,\n",
            "        -0.07381656,  0.18912698],\n",
            "       [ 0.03717782,  0.0909687 ,  0.06755083, ..., -0.02225969,\n",
            "         0.00071458,  0.02340977],\n",
            "       [ 0.06017612, -0.06607175, -0.08958562, ..., -0.02279885,\n",
            "        -0.11828368,  0.14664797]], dtype=float32)>, 'out': <tf.Variable 'weights/weight_4:0' shape=(64, 10) dtype=float32, numpy=\n",
            "array([[ 0.04140841,  0.02620816,  0.03979668, -0.07881442,  0.07606588,\n",
            "        -0.01456708, -0.01214166,  0.03829337, -0.18784744, -0.12745874],\n",
            "       [ 0.07452415,  0.07794728,  0.01297515,  0.16762982, -0.11501762,\n",
            "        -0.0961213 ,  0.03599808,  0.01335052, -0.08470625, -0.03653782],\n",
            "       [ 0.12999101, -0.11723673, -0.03772512,  0.06489661, -0.01011231,\n",
            "        -0.03108566, -0.02426545, -0.11808368, -0.07137866, -0.05140027],\n",
            "       [ 0.17473115, -0.09226886, -0.07224759,  0.03111574,  0.02494979,\n",
            "        -0.16977024, -0.06892129, -0.14050977,  0.08085615, -0.04867759],\n",
            "       [-0.08380288, -0.00745974,  0.11925814, -0.06905223, -0.02756503,\n",
            "         0.05638136, -0.03709237, -0.0853991 , -0.03710161, -0.02127363],\n",
            "       [-0.02833352, -0.02748497,  0.13333285,  0.10270835, -0.19905087,\n",
            "        -0.19280393, -0.04632603, -0.05783927, -0.03245703,  0.06239524],\n",
            "       [-0.09657264, -0.11778741, -0.08176773,  0.14058903, -0.03813589,\n",
            "         0.03350741, -0.01690941,  0.10467435, -0.16575094,  0.12615073],\n",
            "       [ 0.08029165, -0.03689749,  0.00020691, -0.05089176,  0.07016941,\n",
            "         0.18527266, -0.09580313, -0.12411388, -0.00652874, -0.1103573 ],\n",
            "       [-0.05378544, -0.00074247,  0.17848754, -0.01922406,  0.09853964,\n",
            "         0.06322368,  0.09677335, -0.05477747, -0.08255989, -0.02057756],\n",
            "       [ 0.09598862,  0.05894765, -0.12815009, -0.17417243,  0.02181864,\n",
            "         0.12199371,  0.05421946, -0.00845031, -0.14691305,  0.07664621],\n",
            "       [-0.06613874, -0.07624217,  0.02284461,  0.05527036, -0.09318012,\n",
            "        -0.15091586, -0.14773487, -0.02901183, -0.09521266, -0.01068429],\n",
            "       [ 0.19445193, -0.078067  ,  0.15542613, -0.10106192,  0.07236857,\n",
            "        -0.10875634, -0.01402381,  0.0193839 , -0.01965832, -0.11519974],\n",
            "       [-0.00880691,  0.0393075 , -0.06402301, -0.04234913,  0.03399634,\n",
            "        -0.05591216, -0.09337813,  0.11463132, -0.0251219 ,  0.00666943],\n",
            "       [ 0.06093658,  0.02733071,  0.10204554, -0.0655914 , -0.00465514,\n",
            "         0.0124132 ,  0.09700293,  0.16875277, -0.04238793,  0.01099535],\n",
            "       [-0.045019  ,  0.0254691 ,  0.08641053,  0.08506574,  0.0311593 ,\n",
            "         0.14753945, -0.01458533,  0.03110043,  0.06618039,  0.0200571 ],\n",
            "       [ 0.1549371 ,  0.07003532,  0.07639945,  0.02060878, -0.11381697,\n",
            "         0.02422201,  0.09841609,  0.17455879, -0.0438141 ,  0.00402369],\n",
            "       [ 0.00292383,  0.17316018,  0.01948238,  0.01280532,  0.00650599,\n",
            "         0.01782921, -0.10543319,  0.03532074,  0.06010429,  0.02152796],\n",
            "       [ 0.09917919,  0.06354921,  0.16621795,  0.06047714, -0.19310525,\n",
            "        -0.08571453,  0.04962087,  0.05057059,  0.03904416, -0.03504173],\n",
            "       [ 0.05758062,  0.04659653, -0.04851843,  0.01110705, -0.14402178,\n",
            "        -0.00653216, -0.09987941,  0.12745494, -0.13912658,  0.01411884],\n",
            "       [ 0.00276394,  0.11768875, -0.10491705,  0.02138415, -0.13234337,\n",
            "         0.04440958, -0.12001427,  0.04138185,  0.03721244,  0.05346455],\n",
            "       [ 0.09291498, -0.14545186,  0.05216778, -0.16646701, -0.04059196,\n",
            "         0.16234246, -0.09561326, -0.05955201,  0.11170226,  0.05721216],\n",
            "       [-0.08009959,  0.04420929,  0.03308994,  0.19726753,  0.02432222,\n",
            "        -0.03362541, -0.13773854, -0.04802622, -0.01593225,  0.01288752],\n",
            "       [-0.10462513,  0.04227933, -0.11718408, -0.0923438 , -0.068909  ,\n",
            "         0.10138746,  0.01746451,  0.03593834, -0.04903489, -0.10807121],\n",
            "       [-0.15462075,  0.10039335,  0.02109932,  0.08151311,  0.06070042,\n",
            "         0.08460807,  0.1527466 ,  0.03618057,  0.12291253,  0.04547769],\n",
            "       [ 0.19076045,  0.10736894,  0.07780888, -0.00203916, -0.05604776,\n",
            "        -0.01705748, -0.002729  ,  0.06485303,  0.1828401 , -0.13500534],\n",
            "       [-0.05941327,  0.03858585, -0.11632242, -0.10295936,  0.17855775,\n",
            "         0.00684948, -0.12979858, -0.03629624, -0.12879328, -0.00822896],\n",
            "       [-0.15268627,  0.08424482,  0.01535319,  0.03383886, -0.05779678,\n",
            "         0.04139179, -0.07622199, -0.09778494,  0.0486952 , -0.13708824],\n",
            "       [ 0.00780048,  0.01640119,  0.12165459,  0.01506351,  0.03166955,\n",
            "        -0.12459781, -0.16426545, -0.02350536,  0.16451895,  0.13609695],\n",
            "       [-0.08952448, -0.03115546, -0.1201181 , -0.01216742,  0.09463947,\n",
            "         0.00718461,  0.11593125, -0.07148182,  0.05743197, -0.13596009],\n",
            "       [ 0.02455976, -0.00120837, -0.05729171,  0.02641529,  0.03024131,\n",
            "        -0.04159349,  0.00659642,  0.02987227,  0.0869739 ,  0.07127304],\n",
            "       [ 0.106393  ,  0.08957738, -0.0240534 , -0.0608844 ,  0.17247067,\n",
            "        -0.01519475, -0.05564576, -0.08941878, -0.13968028,  0.08654468],\n",
            "       [-0.04267024, -0.12172182, -0.00938655,  0.03828096, -0.12753518,\n",
            "         0.09277207,  0.11460354,  0.0172402 ,  0.00370868,  0.10476574],\n",
            "       [ 0.04671998,  0.01226932, -0.01352409,  0.01220779, -0.17794876,\n",
            "        -0.00937587, -0.04338075,  0.13345855, -0.0718096 ,  0.13932353],\n",
            "       [-0.09956256, -0.03517843, -0.05571152,  0.08256748, -0.05984739,\n",
            "        -0.03914143,  0.11113693, -0.0457617 , -0.04671488, -0.01763331],\n",
            "       [-0.0242257 , -0.03941968,  0.11740118, -0.15608732, -0.10109653,\n",
            "        -0.18758759,  0.11320134,  0.01494192, -0.13459948, -0.10493014],\n",
            "       [-0.0683343 , -0.11182896, -0.07163569,  0.07100128, -0.06554153,\n",
            "        -0.12209377,  0.09726797,  0.1907471 , -0.16694672,  0.12186094],\n",
            "       [ 0.04178065,  0.07963096,  0.02972986,  0.09007784,  0.15466008,\n",
            "         0.126533  ,  0.08447222,  0.00455941, -0.06685272,  0.04261571],\n",
            "       [-0.17394735, -0.04667083,  0.03882028,  0.07132548,  0.09731993,\n",
            "        -0.02036711,  0.03611591,  0.17456621, -0.11597294, -0.08228183],\n",
            "       [ 0.02789742,  0.03205075, -0.16882561, -0.07269172, -0.16190083,\n",
            "        -0.09471185, -0.05167986, -0.11414784,  0.00701553, -0.13773678],\n",
            "       [-0.00354613,  0.11169346, -0.04587541,  0.08921873, -0.09322398,\n",
            "        -0.16428344,  0.08460512,  0.01113147, -0.06167242,  0.18505847],\n",
            "       [ 0.08439558, -0.08644926,  0.0295716 ,  0.14973356, -0.08412983,\n",
            "         0.15761441,  0.00119185, -0.05190004,  0.03172314,  0.09781966],\n",
            "       [-0.1471123 , -0.00442209, -0.00328262, -0.02694038, -0.04708895,\n",
            "         0.02031122,  0.00734508, -0.17112803, -0.00374111,  0.07041896],\n",
            "       [-0.08409389, -0.03655654, -0.07886741, -0.15688685,  0.01049728,\n",
            "        -0.00156444,  0.06317361, -0.02540244, -0.07003215,  0.02772214],\n",
            "       [-0.02705637, -0.1396842 , -0.18402918,  0.09890413, -0.15196083,\n",
            "         0.02462319, -0.09171773, -0.03729704,  0.08243197, -0.03416962],\n",
            "       [ 0.01631602,  0.12061872, -0.0951186 ,  0.01867319,  0.07136733,\n",
            "         0.07653791,  0.16695625,  0.0717927 ,  0.13424425,  0.00834306],\n",
            "       [-0.03948322,  0.01159863, -0.07313074, -0.12156422, -0.08362258,\n",
            "        -0.02852133, -0.19579278,  0.06729857, -0.19441774, -0.10225151],\n",
            "       [-0.07684249, -0.10374291,  0.01949802, -0.11280177, -0.0051135 ,\n",
            "         0.0309781 ,  0.11219265,  0.1970495 , -0.16476828, -0.05804925],\n",
            "       [ 0.02891964, -0.05193117,  0.11115817, -0.13996802, -0.08649512,\n",
            "        -0.00099122, -0.01348774,  0.0281325 , -0.13303569, -0.06229971],\n",
            "       [-0.01269966, -0.0446066 , -0.02104885, -0.10878342, -0.00632099,\n",
            "         0.09198531, -0.00523669, -0.00162421, -0.02723649, -0.08648123],\n",
            "       [-0.00274158, -0.13426334,  0.0924807 , -0.04016284,  0.0993246 ,\n",
            "         0.12890479,  0.01382593,  0.03882855, -0.10735369, -0.01942309],\n",
            "       [ 0.05085915,  0.08788721,  0.03004229,  0.00549347, -0.17150933,\n",
            "         0.05938135,  0.10000658,  0.1550833 , -0.00326386,  0.00453079],\n",
            "       [-0.05748984, -0.13393301, -0.06210637, -0.01354739,  0.04721011,\n",
            "        -0.00613136, -0.00402345,  0.06834932, -0.04966178, -0.06952889],\n",
            "       [-0.13217837,  0.05027067,  0.17760643,  0.03931899, -0.10917413,\n",
            "         0.02742458,  0.06333259, -0.09268229, -0.0548631 , -0.06316677],\n",
            "       [-0.06738798,  0.113138  ,  0.0655727 , -0.08226251,  0.04318487,\n",
            "        -0.13793896, -0.05717766,  0.12259797, -0.01988826,  0.09007143],\n",
            "       [-0.14214163,  0.13668443,  0.11945537,  0.00800561,  0.16448514,\n",
            "         0.11017976, -0.13901484,  0.03586089, -0.02075095, -0.07678764],\n",
            "       [-0.04236634,  0.02479683,  0.05494136,  0.0541027 ,  0.08326112,\n",
            "        -0.13869454,  0.1606851 , -0.01959761, -0.07715861,  0.10066157],\n",
            "       [ 0.05985227, -0.11916327,  0.04135863, -0.09140895, -0.11305747,\n",
            "        -0.1532843 ,  0.002498  , -0.16519426,  0.13993761, -0.0488895 ],\n",
            "       [ 0.04299127,  0.09036236,  0.03026162, -0.04921016,  0.08452664,\n",
            "         0.04314869,  0.09472311, -0.06557152, -0.08856553, -0.01055828],\n",
            "       [-0.13041697,  0.12533705,  0.0251034 ,  0.08720297,  0.14602505,\n",
            "         0.07884564, -0.06389994,  0.12108793,  0.04708446, -0.01654263],\n",
            "       [ 0.0536516 , -0.00213857,  0.08889898,  0.0646797 ,  0.12133919,\n",
            "         0.01533912, -0.06747647, -0.02990936,  0.0541858 , -0.09755222],\n",
            "       [ 0.02901568, -0.06123006,  0.05435093,  0.11545392,  0.00747338,\n",
            "        -0.09416899, -0.096577  , -0.18516482, -0.0081542 ,  0.05207489],\n",
            "       [ 0.11622411, -0.101298  ,  0.02297362,  0.11881208, -0.10753535,\n",
            "        -0.01939586, -0.09829777,  0.15343948, -0.1353446 ,  0.01565045],\n",
            "       [ 0.05712719, -0.07877836,  0.12912013, -0.02121076,  0.0033334 ,\n",
            "         0.00236169,  0.09975943, -0.02134848, -0.01199131,  0.08487705],\n",
            "       [ 0.11521491,  0.0860743 ,  0.02911828,  0.07964667,  0.02172929,\n",
            "         0.10986612, -0.02936661,  0.1573066 , -0.1647415 , -0.15656444]],\n",
            "      dtype=float32)>}\n"
          ]
        }
      ],
      "source": [
        "with tf.name_scope('weights'):\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random.truncated_normal([num_input, num_hidden1], stddev=0.1), name='weight_1'),\n",
        "        'w2': tf.Variable(tf.random.truncated_normal([num_hidden1, num_hidden2], stddev=0.1), name='weight_2'),\n",
        "        'w3': tf.Variable(tf.random.truncated_normal([num_hidden2, num_hidden3], stddev=0.1), name='weight_3'),\n",
        "        'out': tf.Variable(tf.random.truncated_normal([num_hidden3, num_output], stddev=0.1), name='weight_4'),\n",
        "    }\n",
        "\n",
        "print(weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E4k_VLzgMN7"
      },
      "source": [
        "The dimension of bias should be a number of neurons in the current layer. For instance, the\n",
        "dimension of bias b2 is the number of neurons in the hidden layer 2. We set the bias value\n",
        "as constant 0.1 in all layers:\n",
        "\n",
        "## Explain code \n",
        "- biases = { ... }:\n",
        "This dictionary biases contains TensorFlow variables (tf.Variable) representing biases for different layers of the neural network.\n",
        "\n",
        "\n",
        "'b1':\n",
        "\n",
        "tf.Variable(tf.constant(0.1, shape=[num_hidden1]), name='bias_1')\n",
        "tf.constant(0.1, shape=[num_hidden1]): Initializes a constant tensor with value 0.1 and shape [num_hidden1]. This sets the initial bias values to 0.1 for the first hidden layer.\n",
        "tf.Variable(...): Converts the constant tensor into a TensorFlow variable.\n",
        "name='bias_1': Specifies the name of the variable in TensorFlow's graph.\n",
        "\n",
        "'b2', 'b3', 'out':\n",
        "\n",
        "These follow a similar pattern as 'b1'.\n",
        "They initialize biases for the second hidden layer (b2), third hidden layer (b3), and the output layer (out) respectively.\n",
        "Each bias tensor is initialized with a constant value of 0.1 and has a shape corresponding to the number of neurons in its respective layer (num_hidden2, num_hidden3, num_output).\n",
        "\n",
        "## Purpose of Biases in Neural Networks:\n",
        "- Biases are additional parameters in neural networks that allow the model to fit the data better and make the network more flexible in its predictions.\n",
        "- Each neuron typically has its own bias, which is added to the weighted sum of inputs before applying the activation function.\n",
        "- Biases help in shifting the activation function to better fit the data, especially when the weighted sum of inputs is zero or negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "6a8i9LPogMN7"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('biases'):\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n",
        "        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n",
        "        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden3]),name='bias_3'),\n",
        "        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVpndEC5gMN7"
      },
      "source": [
        "## Forward Propagation\n",
        "\n",
        "Now, we define the forward propagation operation. We use relu activations in all layers\n",
        "and in the last layer we use sigmoid activation as defined below:\n",
        "\n",
        "## Summay the code below:\n",
        "- The provided code snippet constructs a sequential neural network model using TensorFlow's Keras API. It consists of three hidden layers with ReLU activation and an output layer with sigmoid activation. Each layer's configuration defines the number of neurons and activation function used, thereby creating a network capable of learning and making predictions based on the provided data inputs.\n",
        "\n",
        "## Explain detail code \n",
        "\n",
        "model = tf.keras.Sequential([...]):\n",
        "\n",
        "This line creates a sequential model in TensorFlow using tf.keras.Sequential.\n",
        "Sequential models are linear stacks of layers, where each layer has exactly one input tensor and one output tensor.\n",
        "Layers in the Sequential Model:\n",
        "\n",
        "tf.keras.layers.Dense(num_hidden1, activation='relu', input_shape=(num_input,), name='layer1'):\n",
        "\n",
        "This defines the first layer (layer1) of the model.\n",
        "tf.keras.layers.Dense: This indicates a fully connected (dense) layer, where each neuron is connected to every neuron in the previous layer.\n",
        "num_hidden1: Number of neurons in the layer. This layer has num_hidden1 neurons.\n",
        "activation='relu': Activation function used in this layer is ReLU (Rectified Linear Unit), which helps introduce non-linearity.\n",
        "input_shape=(num_input,): Specifies the shape of input data expected by this layer. For the first layer, this parameter is required and defines the shape of input data.\n",
        "name='layer1': Name of the layer in the TensorFlow graph.\n",
        "tf.keras.layers.Dense(num_hidden2, activation='relu', name='layer2'):\n",
        "\n",
        "This defines the second layer (layer2) of the model.\n",
        "Similar to the first layer, but without input_shape since it follows layer1.\n",
        "num_hidden2: Number of neurons in the second hidden layer.\n",
        "activation='relu': ReLU activation function is used again.\n",
        "tf.keras.layers.Dense(num_hidden3, activation='relu', name='layer3'):\n",
        "\n",
        "Defines the third layer (layer3) of the model.\n",
        "Similar to the previous layers with num_hidden3 neurons and ReLU activation.\n",
        "tf.keras.layers.Dense(num_output, activation='sigmoid', name='output_layer'):\n",
        "\n",
        "Defines the output layer (output_layer) of the model.\n",
        "num_output: Number of neurons in the output layer. Typically, this corresponds to the number of classes in a classification problem.\n",
        "activation='sigmoid': Sigmoid activation function is used in the output layer, which is suitable for binary classification tasks where each output neuron represents a class probability.\n",
        "Purpose of Each Layer:\n",
        "\n",
        "\n",
        "## Purpose of Each Layer:\n",
        "- Input Layer (layer1):\n",
        "\n",
        "Receives input data with shape (num_input,).\n",
        "- Applies ReLU activation to introduce non-linearity.\n",
        "Hidden Layers (layer2, layer3):\n",
        "\n",
        "Process the input from previous layers using num_hidden2 and num_hidden3 neurons respectively.\n",
        "Each layer applies ReLU activation function to learn complex patterns in the data.\n",
        "Output Layer (output_layer):\n",
        "\n",
        "- Produces the final output predictions.\n",
        "Uses sigmoid activation to produce output values between 0 and 1, suitable for binary classification or multi-label classification where each output can be interpreted as a class probability.\n",
        "Summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "9VptXnxmgMN8"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_hidden1, activation='relu', input_shape=(num_input,), name='layer1'),\n",
        "    tf.keras.layers.Dense(num_hidden2, activation='relu', name='layer2'),\n",
        "    tf.keras.layers.Dense(num_hidden3, activation='relu', name='layer3'),\n",
        "    tf.keras.layers.Dense(num_output, activation='sigmoid', name='output_layer')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zou_1AbkgMN8"
      },
      "source": [
        "## Compute Loss and Backpropagate\n",
        "\n",
        "\n",
        "\n",
        "Next, we define our loss function. We use softmax cross-entropy as our loss\n",
        "function. Tensorflow\n",
        "provides tf.nn.softmax_cross_entropy_with_logits() function for computing the\n",
        "softmax cross entropy loss. It takes the two parameters as inputs logits and labels.\n",
        "\n",
        "* logits implies the logits predicted by our network. That is, y_hat\n",
        "\n",
        "* labels imply the actual labels. That is, true labels y\n",
        "\n",
        "\n",
        "We take mean of the loss using tf.reduce_mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "IdDtbt2QgMN9"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',  # Using categorical crossentropy for multi-class classification\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-O61DJgMN-"
      },
      "source": [
        "Now, we need to minimize the loss using backpropagation. Don't worry! We don't have to\n",
        "calculate derivatives of all the weights manually. Instead, we can use tensorflow's\n",
        "optimizer. In this section, we use Adam optimizer. It is a variant of gradient descent\n",
        "optimization technique we learned in the previous chapter. In the next chapter, we will\n",
        "dive into detail and see how exactly all the Adam and several other optimizers work. For\n",
        "now, let's say we use Adam optimizer as our backpropagation algorithm,\n",
        "\n",
        "\n",
        "tf.train.AdamOptimizer() requires the learning rate as input. So we set 1e-4 as the learning rate and we minimize the loss with minimize() function. It computes the gradients and updates the parameters (weights and biases) of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "kPaQG6_HgMN-"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7na_ADfgMN-"
      },
      "source": [
        "##  Compute Accuracy\n",
        "\n",
        "We calculate the accuracy of our model as follows.\n",
        "\n",
        "\n",
        "* y_hat denotes the predicted probability for each class by our model. Since we have 10 classes we will have 10 probabilities. If the probability is high at position 7, then it means that our network predicts the input image as digit 7 with high probability.  tf.argmax() returns the index of the largest value. Thus, tf.argmax(y_hat,1) gives the index where the probability is high. Thus, if the probability is high at index 7, then it returns 7\n",
        "<br>\n",
        "\n",
        "\n",
        "* Y denotes the actual labels and it is the one hot encoded values. That is, it consists of zeros everywhere except at the position of the actual image where it consists of 1. For instance, if the input image is 7, then Y has 0 at all indices except at index 7 where it has 1. Thus, tf.argmax(Y,1) returns 7 because that is where we have high value i.e 1.\n",
        "\n",
        "\n",
        "Thus, tf.armax(y_hat,1) gives the predicted digit and tf.argmax(Y,1) gives us the actual digit.\n",
        "\n",
        "tf.equal(x, y) takes x and y as inputs and returns the truth value of (x == y) element-wise. Thus, correct_pred = tf.equal(predicted_digit,actual_digit) consists of True where the actual and predicted digits are same and False where the actual and predicted digits are not the same. We convert the boolean values in correct_pred into float using tensorflow's cast operation. That is, tf.cast(correct_pred, tf.float32). After converting into float values, take the average using tf.treduce_mean().\n",
        "\n",
        "Thus, tf.reduce_mean(tf.cast(correct_pred, tf.float32)) gives us the average correct predictions.\n",
        "\n",
        "\n",
        "## Explaining code\n",
        "Reshape Method:\n",
        "\n",
        "reshape(-1, 28*28) transforms the array into a 2D array.\n",
        "-1 is a placeholder that tells NumPy to automatically calculate the number of rows. It essentially means \"as many rows as needed based on the size of the original array and the new dimensions.\"\n",
        "28*28 calculates to 784, which means each image (originally a 28x28 2D array) will be flattened into a 1D array of 784 elements.\n",
        "\n",
        "## Sample Code with visible output to understand\n",
        "- Example 3D array (2 images of 2x3 pixels)\n",
        "x_train = np.array([\n",
        "    [[1, 2, 3],\n",
        "     [4, 5, 6]],\n",
        "    \n",
        "    [[7, 8, 9],\n",
        "     [10, 11, 12]]\n",
        "])\n",
        "\n",
        "- Original shape: (2, 2, 3)\n",
        "print(\"Original shape:\", x_train.shape)\n",
        "\n",
        "- Reshaping\n",
        "x_train_flat = x_train.reshape(-1, 2*3)\n",
        "\n",
        "- New shape: (2, 6)\n",
        "print(\"New shape:\", x_train_flat.shape)\n",
        "\n",
        "- Output the reshaped array\n",
        "print(x_train_flat)\n",
        "\n",
        "- Ouput Result \n",
        "Original shape: (2, 2, 3)\n",
        "New shape: (2, 6)\n",
        "[[ 1  2  3  4  5  6]\n",
        " [ 7  8  9 10 11 12]]\n",
        "\n",
        "Here, two 2x3 images are flattened into two 1x6 arrays.\n",
        "\n",
        "Why Reshape?\n",
        "Neural Networks: Many machine learning models, especially fully connected neural networks, expect input as a flat 1D array per sample.\n",
        "Consistency: Ensures consistent input shape for the model, making data handling simpler.\n",
        "Preprocessing: Often used in data preprocessing pipelines to prepare data for various machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "rhFQD28qgMN_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in training set: 60000\n",
            "Number of labels in training set: 60000\n",
            "Number of images in test set: 10000\n",
            "Number of labels in test set: 10000\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - loss: 0.1021 - mse: 0.0232 - val_loss: 0.0248 - val_mse: 0.0068\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0182 - mse: 0.0047 - val_loss: 0.0165 - val_mse: 0.0044\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0122 - mse: 0.0033 - val_loss: 0.0155 - val_mse: 0.0042\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0085 - mse: 0.0023 - val_loss: 0.0154 - val_mse: 0.0038\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 0.0071 - mse: 0.0019 - val_loss: 0.0148 - val_mse: 0.0038\n",
            "313/313 - 1s - 2ms/step - loss: 0.0148 - mse: 0.0038\n",
            "\n",
            "Test accuracy: 0.0037568954285234213\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape the data to flatten each image\n",
        "x_train_flat = x_train.reshape(-1, 28*28)\n",
        "x_test_flat = x_test.reshape(-1, 28*28)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print the number of images and labels in the training and test sets\n",
        "print(\"Number of images in training set: {}\".format(x_train.shape[0]))\n",
        "print(\"Number of labels in training set: {}\".format(y_train.shape[0]))\n",
        "\n",
        "print(\"Number of images in test set: {}\".format(x_test.shape[0]))\n",
        "print(\"Number of labels in test set: {}\".format(y_test.shape[0]))\n",
        "\n",
        "# Define network parameters\n",
        "num_input = 784  # 28*28\n",
        "num_hidden1 = 256\n",
        "num_hidden2 = 128\n",
        "num_hidden3 = 64\n",
        "num_hidden4 = 32\n",
        "num_output = 10  # number of classes\n",
        "\n",
        "# Define the model using tf.keras.Sequential\n",
        "\n",
        "with tf.name_scope('weights'):\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random.truncated_normal([num_input, num_hidden1], stddev=0.1), name='weight_1'),\n",
        "        'w2': tf.Variable(tf.random.truncated_normal([num_hidden1, num_hidden2], stddev=0.1), name='weight_2'),\n",
        "        'w3': tf.Variable(tf.random.truncated_normal([num_hidden2, num_hidden3], stddev=0.1), name='weight_3'),\n",
        "        'out': tf.Variable(tf.random.truncated_normal([num_hidden3, num_output], stddev=0.1), name='weight_4'),\n",
        "    }\n",
        "\n",
        "with tf.name_scope('biases'):\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n",
        "        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n",
        "        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden3]),name='bias_3'),\n",
        "        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n",
        "    }\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_hidden1, activation='relu', input_shape=(num_input,), name='layer1'),\n",
        "    tf.keras.layers.Dense(num_hidden2, activation='relu', name='layer2'),\n",
        "    tf.keras.layers.Dense(num_hidden3, activation='relu', name='layer3'),\n",
        "    tf.keras.layers.Dense(num_hidden4, activation='relu', name='layer4'),\n",
        "    tf.keras.layers.Dense(num_output, activation='softmax', name='output_layer')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\n",
        "\n",
        "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse']) -> accuracy very low\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_flat, y_train, epochs=5, batch_size=32,\n",
        "          validation_data=(x_test_flat, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=2)\n",
        "print(\"\\nTest accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in training set: 60000\n",
            "Number of labels in training set: 60000\n",
            "Number of images in test set: 10000\n",
            "Number of labels in test set: 10000\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.1104 - precision_2: 0.9083 - recall_2: 0.7584 - val_accuracy: 0.9637 - val_loss: 0.0216 - val_precision_2: 0.9672 - val_recall_2: 0.9619\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0189 - precision_2: 0.9728 - recall_2: 0.9685 - val_accuracy: 0.9700 - val_loss: 0.0172 - val_precision_2: 0.9720 - val_recall_2: 0.9691\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0118 - precision_2: 0.9825 - recall_2: 0.9806 - val_accuracy: 0.9722 - val_loss: 0.0163 - val_precision_2: 0.9736 - val_recall_2: 0.9714\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9847 - loss: 0.0091 - precision_2: 0.9859 - recall_2: 0.9841 - val_accuracy: 0.9786 - val_loss: 0.0140 - val_precision_2: 0.9798 - val_recall_2: 0.9776\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9889 - loss: 0.0068 - precision_2: 0.9896 - recall_2: 0.9884 - val_accuracy: 0.9790 - val_loss: 0.0146 - val_precision_2: 0.9798 - val_recall_2: 0.9783\n",
            "313/313 - 1s - 2ms/step - accuracy: 0.9790 - loss: 0.0146 - precision_2: 0.9798 - recall_2: 0.9783\n",
            "\n",
            "Test accuracy: 0.9789999723434448\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape the data to flatten each image\n",
        "x_train_flat = x_train.reshape(-1, 28*28)\n",
        "x_test_flat = x_test.reshape(-1, 28*28)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print the number of images and labels in the training and test sets\n",
        "print(\"Number of images in training set: {}\".format(x_train.shape[0]))\n",
        "print(\"Number of labels in training set: {}\".format(y_train.shape[0]))\n",
        "\n",
        "print(\"Number of images in test set: {}\".format(x_test.shape[0]))\n",
        "print(\"Number of labels in test set: {}\".format(y_test.shape[0]))\n",
        "\n",
        "# Define network parameters\n",
        "num_input = 784  # 28*28\n",
        "num_hidden1 = 256\n",
        "num_hidden2 = 128\n",
        "num_hidden3 = 64\n",
        "num_hidden4 = 32\n",
        "num_output = 10  # number of classes\n",
        "\n",
        "# Define the model using tf.keras.Sequential\n",
        "\n",
        "with tf.name_scope('weights'):\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random.truncated_normal([num_input, num_hidden1], stddev=0.1), name='weight_1'),\n",
        "        'w2': tf.Variable(tf.random.truncated_normal([num_hidden1, num_hidden2], stddev=0.1), name='weight_2'),\n",
        "        'w3': tf.Variable(tf.random.truncated_normal([num_hidden2, num_hidden3], stddev=0.1), name='weight_3'),\n",
        "        'out': tf.Variable(tf.random.truncated_normal([num_hidden3, num_output], stddev=0.1), name='weight_4'),\n",
        "    }\n",
        "\n",
        "with tf.name_scope('biases'):\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n",
        "        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n",
        "        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden3]),name='bias_3'),\n",
        "        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n",
        "    }\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_hidden1, activation='relu', input_shape=(num_input,), name='layer1'),\n",
        "    tf.keras.layers.Dense(num_hidden2, activation='relu', name='layer2'),\n",
        "    tf.keras.layers.Dense(num_hidden3, activation='relu', name='layer3'),\n",
        "    tf.keras.layers.Dense(num_hidden4, activation='relu', name='layer4'),\n",
        "    tf.keras.layers.Dense(num_output, activation='softmax', name='output_layer')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_flat, y_train, epochs=5, batch_size=32,\n",
        "          validation_data=(x_test_flat, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation = model.evaluate(x_test_flat, y_test, verbose=2)\n",
        "test_loss = evaluation[0]\n",
        "test_acc = evaluation[1]\n",
        "\n",
        "print(\"\\nTest accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN7mdZPggMN_"
      },
      "source": [
        "## Create Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VAyYLB1gMN_"
      },
      "source": [
        "We can also visualize how the loss and accuracy of our model change during several\n",
        "iterations in tensorboard. So, we use tf.summary() to get the summary of the variable.\n",
        "Since the loss and accuracy are scalar variables, we use tf.summary.scalar() to store the summary as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "yc_jjU5ygMN_",
        "outputId": "7a735108-d6c9-4732-c434-e5263110c331"
      },
      "outputs": [],
      "source": [
        "def log_metrics(epoch, logs):\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('Loss', logs['loss'], step=epoch)\n",
        "        tf.summary.scalar('Accuracy', logs['accuracy'], step=epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT9SZ03KgMN_"
      },
      "source": [
        "Next, we merge all the summaries we use in our graph using tf.summary.merge_all(). We merge all summaries because when we have many summaries running and storing them would become inefficient, so we merge all the summaries and run them once in our session instead of running multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "bf8jy2KvgMOA"
      },
      "outputs": [],
      "source": [
        "log_dir = './logs'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAn8sOVgMOA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RAiFKaUgMOA"
      },
      "source": [
        "Now it is time to train our model. As we learned, first we need to initialize all the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "7S5OxzHegMOA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 34.74552    -49.10192     31.853767   ... -28.479866    33.149857\n",
            "  -47.112587  ]\n",
            " [ 12.811256    -0.719841    21.735346   ...  16.905941   -41.464085\n",
            "   14.001776  ]\n",
            " [  2.1387434  -49.768616    23.079247   ...  22.070805   -14.569541\n",
            "   -7.1683664 ]\n",
            " ...\n",
            " [ 39.19709     17.964693    28.049442   ...  -8.094973     6.6966715\n",
            "   -0.42750788]\n",
            " [ 23.667973   -18.910034     8.565903   ...  25.022198   -15.401224\n",
            "   27.603283  ]\n",
            " [ -7.1534195  -62.53518      7.9484005  ...  50.550648   -26.955359\n",
            "  -25.246634  ]], shape=(32, 256), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example explicit variable initialization\n",
        "w = tf.Variable(tf.random.normal(shape=(784, 256)), name='weight')\n",
        "b = tf.Variable(tf.zeros(shape=(256,)), name='bias')\n",
        "\n",
        "# Explicitly assign initial values (if needed)\n",
        "w.assign(tf.random.normal(shape=(784, 256)))\n",
        "b.assign(tf.zeros(shape=(256,)))\n",
        "\n",
        "# Use variables in a model or computation\n",
        "x = tf.random.normal(shape=(32, 784))\n",
        "output = tf.matmul(x, w) + b\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWaZEm7gMOA"
      },
      "source": [
        "Define the batch size and number of iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "oKXWE1iogMOA"
      },
      "outputs": [],
      "source": [
        "batch_size_number = 128\n",
        "num_iterations = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8yiVVWUgMOB"
      },
      "source": [
        "Start the tensorflow session and perform training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "g_dsZT7agMOB",
        "outputId": "b56d4646-dd7e-476d-e982-ce447f8da076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9947 - loss: 0.0166 - val_accuracy: 0.9847 - val_loss: 0.0594\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9985 - loss: 0.0061 - val_accuracy: 0.9838 - val_loss: 0.0679\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 0.9851 - val_loss: 0.0639\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9996 - loss: 0.0024 - val_accuracy: 0.9844 - val_loss: 0.0718\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9827 - val_loss: 0.0850\n",
            "313/313 - 1s - 2ms/step - accuracy: 0.9827 - loss: 0.0850\n",
            "Test accuracy: 0.982699990272522\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime  # For unique log directory per run\n",
        "\n",
        "# Set up TensorBoard for logging\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train model using fit method\n",
        "model.fit(x_train.reshape(-1, 784), y_train, epochs=5, batch_size=batch_size_number,\n",
        "          validation_data=(x_test.reshape(-1, 784), y_test),\n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test.reshape(-1, 784), y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DYv8RB7gMOS"
      },
      "source": [
        "As you may observe, the loss decreases and the accuracy increases over the training iterations. Now that we have learned how to build the neural network using tensorflow, in the next section we will see how can we visualize the computational graph of our model in tensorboard."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
